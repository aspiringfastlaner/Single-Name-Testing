{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing delta probability for weeklies and monthlies\\n\",\n",
    "    - Does delta match probability of occurrence\\n\",\n",
    "    - Check Kelly criterion for long straddles/delta neutral back ratios\\n\",\n",
    "        - To calculate expected losses and gains, use options pricing module to interpolate prices at given percentage\\n\",\n",
    "        moves and use the normal pdf as the probability weightings\\n\n",
    "Check earnings returns post announcement factors\\n\",\n",
    "    - Factors:\\n\",\n",
    "        - Number of times beaten earnings (Dummy Variable)\\n\",\n",
    "        - Consecutive earnings beats\\n\",\n",
    "        - Consecutive earnings upsets\\n\",\n",
    "        - 3 Month Trend before earnings\\n\",\n",
    "        - YTD Trend\\n\",\n",
    "        - Momentum of monthly returns (20 day, 60 day)\\n\"\n",
    "        \n",
    "        \n",
    "price-to-book (P / B), price-to-earnings (P / E), price-to-free cashflow (P / FCF) and enterprise value-to-EBITDA (EV / EBITDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader.data import Options\n",
    "from py_vollib.black_scholes_merton.implied_volatility import *\n",
    "# ts = TimeSeries(key='5HZEUI5AFJB06BUK',output_format='pandas')\n",
    "import py_vollib\n",
    "from py_vollib.black_scholes_merton.implied_volatility import *\n",
    "from py_vollib.black_scholes_merton.greeks.analytical import *\n",
    "import plotly\n",
    "import os\n",
    "import pandas_market_calendars as mcal\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import urllib.request as req\n",
    "import time\n",
    "nyse = mcal.get_calendar('NYSE')\n",
    "from helpers import *\n",
    "from tickers import tickers\n",
    "\n",
    "from scipy.stats import norm as norm\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "from openpyxl import load_workbook\n",
    "from yahoo_query import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# functions list:\n",
    "\n",
    "#     maturities(dt.datetime()) --> [float(front_wgt), float(back_wgt)]\n",
    "\n",
    "#     optionslam_scrape(str[ticker]) --> dict[earnings]\n",
    "\n",
    "#     yahoo_table_parse(str[raw_html_table]) --> DataFrame[ticker]\n",
    "\n",
    "#     yahoo_earnings(dt.datetime()) --> DataFrame[earnings_on_date]\n",
    "\n",
    "#     fundamentals(str[ticker]) --> DataFrame[stock_fundamentals]\n",
    "\n",
    "#     get_fundas(list[ticker_lst]) --> DataFrame[stock_fundamentals]\n",
    "\n",
    "#     historical_data(str[ticker], int[day_number], int[rolling_window], outsize[str]) --> DataFrame[daily_stock_data]\n",
    "\n",
    "#     current_volatility(list[ticker_lst], int[roll]) --> DataFrame[stock_volatilities]\n",
    "\n",
    "#     all_options(str[ticker], bool[greeks]) --> DataFrame[options_chains]\n",
    "\n",
    "#     earnings_condor(str[ticker], int[max_gap], int[dte_thresh], float[|money_thresh| <= 1]) -- DataFrame[condors], DataFrame[puts], DataFrame[calls]\n",
    "\n",
    "#     write_excel(str[filename], list[str[sheetnames]], list[dataframes]) --> void()\n",
    "\n",
    "#     curr_stock_data(str[ticker]) --> DataFrame[stock_info]\n",
    "\n",
    "#     curr_batch_quotes(list_of_string[tickers]) --> DataFrame[stock_info]\n",
    "\n",
    "#     past_earnings(str[ticker]) --> DataFrame[earnings_info]\n",
    "\n",
    "#     earnings_history(str[ticker]) --> [DataFrame[earnings_estimate], DataFrame[past_earnings], DataFrame[earnings_estimate_change]]\n",
    "\n",
    "#     av_data(str[ticker]) --> DataFrame[ticker_open, ticker_close]\n",
    "\n",
    "#     av_batch(list_of_str[tickers]) --> DataFrame[tickers_closes]\n",
    "\n",
    "#     check_mkt_corr(int[corr_rolling_window],int[plot_window]) --> DataFrame[rolling_corr]\n",
    "\n",
    "#     vvix_check() --> DataFrame[VVIX Data]\n",
    "\n",
    "#     earnings_longs(list_of_str[ticker], float[bid_ask_spread]) --> DataFrame[option_chains]\n",
    "\n",
    "#     all_options_v2(str[ticker], int[dte_ub], int[dte_lb], float[moneyness]) --> DataFrame[option_chains]\n",
    "\n",
    "#     yahoo_options_query(str[ticker], int[dte_ub], int[dte_lb]) --> DataFrame[option_chains]\n",
    "\n",
    "#     greek_calc(DataFrame[option_chain], str[prem_price_use], str[day_format], float[interest_rate], float[dividend_rate])\n",
    "\n",
    "#     price_sim(DataFrame[options_df], float[price_change], float[vol_change], int[days_change], str[output = 'All'],\n",
    "#               str[skew = 'flat'], str[day_format = 'trading'], float[interest_rate = 0.0193], float[dividend_rate = 0],\n",
    "#               float[prem_price_use = 'Mid'])\n",
    "\n",
    "\n",
    "#     position_sim(DataFrame[position_df], list_of_int[holdings], int[shares],\n",
    "#                  float[price_change], float[vol_change], int[dte_change], str[output = 'All'],\n",
    "#                  str[skew = 'flat'], str[prem_price_use = 'Mid'], str[day_format = 'trading'], \n",
    "#                  float[interest_rate = 0.0193], float[dividend_rate = 0])\n",
    "\n",
    "#     yahoo_fundamentals(list_of_str[tickers]) --> DataFrame[fundamentals]\n",
    "\n",
    "# '''\n",
    "\n",
    "# stock_list = pd.read_csv('optionablestocks.csv')['OPTION SYMBOL'].tolist()\n",
    "# stock_list = list(set(tickers[3:] + stock_list))\n",
    "\n",
    "cad_names = pd.read_csv('cad_names.csv')['Symbol'].tolist()\n",
    "us_names = pd.read_csv('us_names.csv')['Symbol'].tolist()\n",
    "start_time = time.time()\n",
    "\n",
    "def pull_data(ticker):\n",
    "    yahoo_data = yahoo_query(ticker,dt.datetime(2018,1,1))\n",
    "    yahoo_data.full_info_query()\n",
    "    earnings_info_quarter = yahoo_data.earnings_quarterly.join(yahoo_data.cashFlowStatementQuarter).join(yahoo_data.incomeStatementQuarter.drop(['netIncome','maxAge'],\n",
    "                                                                                                                                      axis = 1),\n",
    "                                                                                               rsuffix='_income').join(yahoo_data.balanceSheetQuarter,\n",
    "                                                                                                                       rsuffix = '_balance')\n",
    "    \n",
    "    annual_info = yahoo_data.cashFlowStatementAnnual.join(yahoo_data.incomeStatementAnnual.drop(['netIncome','maxAge'],\n",
    "                                                                                                  axis = 1),\n",
    "                                                           rsuffix='_income').join(yahoo_data.balanceSheetAnnual,\n",
    "                                                                                   rsuffix = '_balance')\n",
    "    annual_info['earnings'] = yahoo_data.earnings_annual.sort_index(ascending = False)['earnings'].tolist()\n",
    "#     earnings_info['earnBeatsBefore'] = 0\n",
    "#     earnings_info['earnMissBefore'] = 0\n",
    "\n",
    "#     for idx, row in earnings_info.iterrows():\n",
    "#         earnings_info.loc[idx,'earnBeatsBefore'] = len(earnings_info[(earnings_info.index <= idx) & (earnings_info.epsDifference > 0)])\n",
    "#         earnings_info.loc[idx,'earnMissBefore'] = len(earnings_info[(earnings_info.index <= idx) & (earnings_info.epsDifference <= 0)])\n",
    "#     earnings_info = earnings_info.shift(1)\n",
    "\n",
    "\n",
    "#     earnings_moves = past_earnings(ticker).sort_index()\n",
    "#     earnings_moves = earnings_moves[(earnings_moves.index > min(yahoo_data.earnings_history.index) - dt.timedelta(days = 92)) &\n",
    "#                                     (earnings_moves.index <= max(yahoo_data.earnings_history.index))].sort_index()\n",
    "\n",
    "\n",
    "#     earnings_df = pd.concat([earnings_info.reset_index(), \n",
    "#                              earnings_moves.reset_index()], axis = 1)\n",
    "    earnings_info_quarter.columns = ['quarter' if col == 'index' else col for col in earnings_info_quarter.columns.tolist()]\n",
    "    earnings_info_quarter['Underlying'] = ticker\n",
    "    annual_info.columns = ['year' if col == 'index' else col for col in annual_info.columns.tolist()]\n",
    "    annual_info['Underlying'] = ticker\n",
    "\n",
    "    ### separate df for current key measures\n",
    "    keyMetrics = yahoo_data.profile.join(yahoo_data.keyStats).join(yahoo_data.finData, rsuffix = '_finData')\n",
    "\n",
    "    return (earnings_info_quarter, annual_info, keyMetrics)\n",
    "\n",
    "def download_yahoo_data(ticker_list, retries = 10):\n",
    "\n",
    "    earnings_lst = []\n",
    "    annual_lst = []\n",
    "    keyStats_lst = []\n",
    "\n",
    "    item_counter = 0\n",
    "    total_length = len(ticker_list)\n",
    "    failed_list = []\n",
    "\n",
    "    for ticker in ticker_list:\n",
    "        try:\n",
    "            curr_earnings, curr_annual_info, curr_keyStats = pull_data(ticker)\n",
    "            earnings_lst.append(curr_earnings)\n",
    "            annual_lst.append(curr_annual_info)\n",
    "            keyStats_lst.append(curr_keyStats)\n",
    "            print('Accepted: {}'.format(ticker))\n",
    "        except:\n",
    "            for i in range(retries):\n",
    "                try:\n",
    "                    curr_earnings, curr_annual_info, curr_keyStats = pull_data(ticker)\n",
    "                    earnings_lst.append(curr_earnings)\n",
    "                    annual_lst.append(curr_annual_info)\n",
    "                    keyStats_lst.append(curr_keyStats)\n",
    "                    print('Accepted: {}'.format(ticker))\n",
    "                except:\n",
    "                    continue\n",
    "            print('Failed: {}'.format(ticker))\n",
    "            failed_list.append(ticker)\n",
    "\n",
    "        item_counter += 1\n",
    "        print('{0:.2f}% Completed'.format(item_counter/total_length*100))\n",
    "        print('{} total failures'.format(len(failed_list)))\n",
    "\n",
    "    earnings_df = pd.concat(earnings_lst, axis = 0)\n",
    "    annual_df = pd.concat(annual_lst, axis = 0)\n",
    "    #earnings_df = earnings_df.reset_index()[earnings_df.columns]\n",
    "    keyStats_df = pd.concat(keyStats_lst, axis = 0)\n",
    "\n",
    "    return earnings_df, annual_df, keyStats_df, failed_list\n",
    "\n",
    "def fin_ratios(earnings_df):\n",
    "\n",
    "    ratios_df = earnings_df[['Underlying','timestamp','quarter','1Year', '1month', '3month', '6month', \n",
    "                             'PostEarningsReturn','industry', 'sector']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_df, annual_df, keyStats_df, failed_list = download_yahoo_data(us_names, retries = 3)\n",
    "\n",
    "datenow = dt.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "earnings_df.to_csv('us_quarterly_{}.csv'.format(datenow))\n",
    "annual_df.to_csv('us_annual_{}.csv'.format(datenow))\n",
    "keyStats_df.to_csv('us_keystats_{}.csv'.format(datenow))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earnings_df, annual_df, keyStats_df, failed_list = download_yahoo_data(cad_names, retries = 3)\n",
    "\n",
    "datenow = dt.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "earnings_df.to_csv('cad_quarterly_{}.csv'.format(datenow))\n",
    "annual_df.to_csv('cad_annual_{}.csv'.format(datenow))\n",
    "keyStats_df.to_csv('cad_keystats_{}.csv'.format(datenow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# earnings, ks, failed_list = download_yahoo_data(stock_list, retries = 10)\n",
    "# earnings = earnings.drop(earnings.isnull().sum().sort_values().tail(13).index.tolist(), axis = 1)\n",
    "# earnings.to_csv('full_data.csv')\n",
    "\n",
    "fs_df = pd.read_csv('full_data.csv', index_col = 0)\n",
    "\n",
    "fs = fs_df[['Underlying']]\n",
    "fs['curr_ratio'] = fs_df['totalCurrentAssets']/fs_df['totalCurrentLiabilities']\n",
    "fs['totaldebtequity'] = fs_df['totalLiab']/fs_df['totalStockholderEquity']\n",
    "fs['longdebtequity'] = fs_df['longTermDebt']/fs_df['totalStockholderEquity']\n",
    "fs['shortdebtequity'] = fs_df['shortLongTermDebt']/fs_df['totalStockholderEquity']\n",
    "fs['daySalesOut'] = (fs_df['netReceivables']/fs_df['totalRevenue'].replace(0,np.nan))*365\n",
    "fs['intangToBook'] = fs_df['intangibleAssets']/fs_df['totalStockholderEquity']\n",
    "fs['invToRev'] = fs_df['inventory']/fs_df['totalRevenue'].replace(0,np.nan)\n",
    "fs['ltdebtToCap'] = fs_df['longTermDebt']/fs_df['investments']\n",
    "fs['stdebtToCap'] = fs_df['shortLongTermDebt']/fs_df['investments']\n",
    "fs['ltdebtPercent'] = fs_df['longTermDebt']/fs_df['totalLiab']\n",
    "fs['stdebtPercent'] = fs_df['shortLongTermDebt']/fs_df['totalLiab']\n",
    "\n",
    "fs['grossMargin'] = fs_df['grossProfit']/fs_df['totalRevenue'].replace(0,np.nan)\n",
    "fs['profitMargin'] = fs_df['netIncome']/fs_df['totalRevenue'].replace(0,np.nan)\n",
    "fs['operatingMargin'] = fs_df['operatingIncome']/fs_df['totalRevenue'].replace(0,np.nan)\n",
    "\n",
    "fs['operatingCashflow'] = fs_df['totalCashFromOperatingActivities']/fs_df['totalRevenue'].replace(0,np.nan)\n",
    "fs['assetEfficiency'] = fs_df['totalCashFromOperatingActivities']/fs_df['totalAssets']\n",
    "fs['currLiabCoverage'] = fs_df['totalCashFromOperatingActivities']/fs_df['totalCurrentLiabilities']\n",
    "fs['ltDebtCoverage'] = fs_df['totalCashFromOperatingActivities']/fs_df['longTermDebt']\n",
    "fs['cashGeneratingPower'] = fs_df['totalCashFromOperatingActivities']/(fs_df['totalCashFromOperatingActivities'] + fs_df['totalCashFromFinancingActivities'] + fs_df['totalCashflowsFromInvestingActivities'])\n",
    "\n",
    "fs_pct = []\n",
    "for name in fs.Underlying.drop_duplicates().tolist():\n",
    "    pct = fs[fs['Underlying'] == name].sort_index()\n",
    "    pct[pct.columns.tolist()[1:]] = pct[pct.columns.tolist()[1:]].pct_change()\n",
    "    fs_pct.append(pct.tail(len(pct)-1))\n",
    "\n",
    "fs_pct = pd.concat(fs_pct, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'Leaps Filtering.xlsx'\n",
    "ks = pd.read_excel(filename, sheetname = 'keystats', index_col = 0).drop_duplicates()\n",
    "\n",
    "book = load_workbook(filename)\n",
    "writer = pd.ExcelWriter(filename, engine='openpyxl') \n",
    "writer.book = book\n",
    "writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "\n",
    "fs_pct.drop_duplicates().to_excel(writer, \"finpctdata\")\n",
    "fs.drop_duplicates().to_excel(writer,\"findata\")\n",
    "ks.drop('industry',axis = 1).groupby('sector').mean().to_excel(writer, \"keystatsAvg\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Checking Key Stats on Single Names\n",
    "### For each industry filter names that have:\n",
    "1. positive earnings growth\n",
    "2. positive profit margin\n",
    "3. lower than industry average p/b, p/e, ev/ebitda, debt/equity\n",
    "4. rising margins quarter over quarter\n",
    "5. decreasing debt/equity quarter over quarter\n",
    "6. decreasing days of sales outstanding\n",
    "7. lower than industry average inventory/revenue ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'Leaps Filtering.xlsx'\n",
    "ks = pd.read_excel(filename, sheetname = 'keystats', index_col = 0).drop_duplicates()\n",
    "ksavg = pd.read_excel(filename, sheetname = 'keystatsAvg', index_col = 0).drop_duplicates()\n",
    "finpct = pd.read_excel(filename, sheetname = 'finpctdata', index_col = 0).drop_duplicates().fillna(0)\n",
    "fin = pd.read_excel(filename, sheetname = 'findata', index_col = 0).drop_duplicates()\n",
    "\n",
    "# Creating scores of pct changes\n",
    "tickers = finpct.Underlying.drop_duplicates().tolist()\n",
    "columns = finpct.columns.tolist()[1:]\n",
    "values = ['inc','dec','dec','dec','dec','flat','dec','dec','dec',\n",
    "          'dec','dec','inc','inc','inc','inc','inc','inc','inc',\n",
    "          'inc']\n",
    "field_directions = dict(zip(columns,values))\n",
    "scores = pd.DataFrame(index = tickers, columns = columns)\n",
    "\n",
    "for name in tickers:\n",
    "    for field in columns:\n",
    "        curr_fin = finpct[(finpct['Underlying'] == name)][field]\n",
    "        if len(curr_fin) > 1:\n",
    "            if field_directions[field] == 'inc':\n",
    "                point = len(curr_fin) == sum(curr_fin >= 0)\n",
    "            elif field_directions[field] == 'dec':\n",
    "                point = len(curr_fin) == sum(curr_fin <= 0)\n",
    "            else:\n",
    "                point = False\n",
    "        else:\n",
    "            point = False\n",
    "            \n",
    "        scores.loc[name,field] = point*1\n",
    "        \n",
    "scores['Score'] = scores.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_names = scores[scores['Score'] >= 10].sort_values('Score', ascending = False)\n",
    "ks_col_order = ['currentPrice', 'sector', 'industry',\n",
    "                '52WeekChange', 'SandP52WeekChange', 'beta', 'bookValue',\n",
    "                'currentRatio', 'debtToEquity', 'earningsGrowth', 'revenueGrowth', \n",
    "                'returnOnAssets', 'returnOnEquity',\n",
    "                'earningsQuarterlyGrowth', 'heldPercentInsiders', 'heldPercentInstitutions', \n",
    "                'sharesPercentSharesOut', 'shortPercentOfFloat', \n",
    "                'ebitda', 'ebitdaMargins',\n",
    "                'enterpriseToEbitda', 'enterpriseToRevenue', 'enterpriseValue',\n",
    "                'floatShares', 'forwardEps', 'forwardPE', 'freeCashflow',\n",
    "                'fullTimeEmployees', 'grossMargins', 'grossProfits',\n",
    "                'netIncomeToCommon', 'operatingCashflow', 'profitMargins','operatingMargins',\n",
    "                'overallRisk', 'pegRatio', 'priceToBook', \n",
    "                'quickRatio', 'recommendationKey',\n",
    "                'recommendationMean', \n",
    "                'revenuePerShare', 'sharesOutstanding',\n",
    "                'sharesShort', 'sharesShortPriorMonth',\n",
    "                'shortRatio', 'targetHighPrice',\n",
    "                'targetLowPrice', 'targetMeanPrice', 'targetMedianPrice', 'totalCash',\n",
    "                'totalCashPerShare', 'totalDebt', 'totalRevenue', 'trailingEps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filteredfilename = 'filtered leaps.xlsx'\n",
    "top_names.to_excel(filteredfilename,'scores')\n",
    "\n",
    "book = load_workbook(filteredfilename)\n",
    "writer = pd.ExcelWriter(filteredfilename, engine='openpyxl') \n",
    "writer.book = book\n",
    "writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "\n",
    "finpct[finpct.Underlying.isin(top_names.index.tolist())].to_excel(writer, 'finpctdata')\n",
    "fin[fin.Underlying.isin(top_names.index.tolist())].to_excel(writer, 'findata')\n",
    "ks[ks_col_order][ks.index.isin(top_names.index.tolist())].to_excel(writer,'keystats')\n",
    "ks.drop('industry',axis = 1).groupby('sector').mean().to_excel(writer, \"keystatsAvg\")\n",
    "\n",
    "writer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
